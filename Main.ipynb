{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa1a2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 11:09:27.503 INFO    numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-11-15 11:09:27.504 INFO    numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RTCConfiguration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-468df88864ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# # Convert RGB to BGR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# open_cv_image = open_cv_image[:, :, ::-1].copy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m RTC_CONFIGURATION = RTCConfiguration(\n\u001b[0m\u001b[0;32m     25\u001b[0m     {\n\u001b[0;32m     26\u001b[0m       \"RTCIceServer\": [{\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RTCConfiguration' is not defined"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tensorflow import keras\n",
    "import cv2\n",
    "import os\n",
    "from streamlit_cropper import st_cropper\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from streamlit_webrtc import VideoTransformerBase, webrtc_streamer, RTCConfiguration\n",
    "import av\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# https://github.com/tensorflow/tensorflow/issues/45236\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something like state\n",
    "# https://docs.streamlit.io/library/advanced-features/session-state\n",
    "if 'img' not in st.session_state:\n",
    "    st.session_state.img = []\n",
    "\n",
    "if 'heads' not in st.session_state:\n",
    "    st.session_state.heads = []\n",
    "    \n",
    "if 'headsbool' not in st.session_state:\n",
    "    st.session_state.headsbool = []\n",
    "    \n",
    "if 'face_cascade' not in st.session_state:\n",
    "    st.session_state.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4496fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get head by CascadeClassifier\n",
    "# https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81\n",
    "def get_heads(img):\n",
    "    heads = []\n",
    "    # Convert into grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    faces = st.session_state.face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    # Draw rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        newx = int(x-(w*0.1))\n",
    "        neww = int(w*1.2)\n",
    "        newy = int(y-(h*0.3))\n",
    "        newh = int(h*1.4)\n",
    "        if newx < 0:\n",
    "            newx = 0\n",
    "        if newy < 0:\n",
    "            newy = 0\n",
    "        if newy+newh > img.shape[0]:\n",
    "            newh = img.shape[0] - newy-1\n",
    "        if newx+neww > img.shape[1]:\n",
    "            neww = img.shape[1] - newx-1\n",
    "#         cv2.rectangle(img, (newx, newy), (newx+neww, newy+newh), (255, 0, 0), 2)\n",
    "#         cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        heads.append(img[\n",
    "            int(newy):int(newy+newh),\n",
    "            int(newx):int(newx+neww) \n",
    "        ])\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = st.sidebar.radio(\"Type\",('Real time', 'Upload picture'))\n",
    "if main == 'Upload picture':\n",
    "    progress = st.sidebar.radio(\"Progress\",('Upload Image', 'Process Image', 'Result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransformer(VideoTransformerBase):\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        self.faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')\n",
    "        self.age_model = tf.keras.models.load_model('./model/AgeDetection/Freeze/Freeze_BestModelAge.h5')\n",
    "        self.labels_age = {0: 'Adolescence', 1: 'Adult',2:'Child',3:'Senior Citizen'}\n",
    "        self.gender_model = tf.keras.models.load_model('./model/GenderDetection/Freeze/Freeze_BestModelGender.h5')\n",
    "        self.labels_gender = {0: 'Female', 1: 'Male'}\n",
    "\n",
    "    def transform(self, frame):\n",
    "        # get image and all faces\n",
    "        img = frame.to_ndarray(format=\"bgr24\")\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.faceCascade.detectMultiScale(gray, 1.3, 5)\n",
    "        i =self.i+1\n",
    "        # loop all faces to predict and show on camera\n",
    "        for (x, y, w, h) in faces:\n",
    "            head = img[\n",
    "                int(y):int(y+h),\n",
    "                int(x):int(x+w) \n",
    "            ]\n",
    "#             head = head/255\n",
    "            resized = cv2.resize(head, (224,224))\n",
    "            reshaped = resized.reshape(1,224, 224,3)\n",
    "            reshaped=preprocess_input(reshaped)\n",
    "            predictions = self.age_model(reshaped).numpy()\n",
    "            predicted_class = np.argmax(predictions,axis=1).item(0)\n",
    "            age_predicted_label = self.labels_age[predicted_class]\n",
    "\n",
    "            predictions = self.gender_model(reshaped).numpy()\n",
    "            predicted_class = np.argmax(predictions,axis=1).item(0)\n",
    "            gender_predicted_label = self.labels_gender[predicted_class]\n",
    "\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (95, 207, 30), 3)\n",
    "            cv2.rectangle(img, (x, y - 40), (x + w, y), (95, 207, 30), -1)\n",
    "            cv2.putText(img, age_predicted_label + \"(\" + gender_predicted_label + \")\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "        return img\n",
    "    \n",
    "#     19302\n",
    "if(main == 'Real time'):\n",
    "    webrtc_streamer(key=\"example\",\n",
    "                video_processor_factory=VideoTransformer,\n",
    "                rtc_configuration={\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]},\n",
    "                media_stream_constraints={\n",
    "                    \"audio\": False,\n",
    "                    \"video\": True,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(main == 'Upload picture' and progress == 'Upload Image'):\n",
    "    st.title(\"Upload Image\")\n",
    "    file = st.file_uploader(label='Upload file', type=['png', 'jpg'])\n",
    "    if (file):\n",
    "        image = Image.open(file)\n",
    "        if image.format is 'PNG':\n",
    "            image = image.convert('RGB')\n",
    "        img_array = np.array(image)\n",
    "        st.session_state.img = img_array\n",
    "        st.session_state.headsbool = []\n",
    "        st.session_state.heads = []\n",
    "\n",
    "    if (len(st.session_state.img) != 0):\n",
    "        st.write(\"Click “Process Image” radio button on the side bar when the image is uploaded\")\n",
    "        st.image(st.session_state.img)\n",
    "        \n",
    "        st.session_state.heads = get_heads(st.session_state.img)\n",
    "        for i in range(len(get_heads(st.session_state.img))):\n",
    "            st.session_state.headsbool.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(main == 'Upload picture' and progress == 'Process Image'):\n",
    "    st.title(\"Process Image\")\n",
    "    if (len(st.session_state.img) == 0):\n",
    "        st.error('No Image choosed')\n",
    "    else:\n",
    "        st.write(\"Number of face detected:\", len( get_heads(st.session_state.img)))\n",
    "        st.write(\"detected face have been load to choosed image\")\n",
    "        \n",
    "        box_color = st.color_picker(label=\"Box Color\", value='#0000FF')\n",
    "        img = Image.fromarray(st.session_state.img)\n",
    "#         Get a cropped image from the frontend\n",
    "#         https://github.com/turner-anderson/streamlit-cropper\n",
    "        cropped_img = st_cropper(img, realtime_update=True, box_color=box_color,\n",
    "                                    aspect_ratio=(1,1))\n",
    "        savebtn = st.button(\"save\")\n",
    "        if savebtn:\n",
    "            st.session_state.heads.append(cv2.cvtColor(np.array(cropped_img)[:, :, ::-1].copy(), cv2.COLOR_RGB2BGR))\n",
    "            st.session_state.headsbool.append(True)\n",
    "        # Manipulate cropped image at will\n",
    "        st.header(\"Preview\")\n",
    "        _ = cropped_img.thumbnail((150,150))\n",
    "        st.image(cropped_img)\n",
    "        \n",
    "        st.header(\"Choosed head\")\n",
    "        for i in range(len(st.session_state.heads)):\n",
    "            if (st.session_state.headsbool[i] == True):\n",
    "                placeholder = st.empty()\n",
    "                col1, col2, col3 = placeholder.columns(3)\n",
    "                with col1:\n",
    "                    st.image(st.session_state.heads[i])\n",
    "                with col2:\n",
    "                    if st.button(\"Delete\",key = \"Delete_\"+str(i)):\n",
    "                        placeholder.empty()\n",
    "                        st.session_state.headsbool[i] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c1e45bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'progress' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a2e579284f37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogress\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Result'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No Image choosed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheadsbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'progress' is not defined"
     ]
    }
   ],
   "source": [
    "if(main == 'Upload picture' and progress == 'Result'):\n",
    "    st.title(\"Result\")\n",
    "    if (len(st.session_state.img) == 0):\n",
    "        st.error('No Image choosed')\n",
    "    if (np.count_nonzero(st.session_state.headsbool) == 0):\n",
    "        st.error('Picture not processed and cannot detect face by CascadeClassifier')\n",
    "    else:\n",
    "        st.image(st.session_state.img)\n",
    "        age_model = tf.keras.models.load_model('./model/AgeDetection/Freeze/Freeze_BestModelAge.h5')\n",
    "        labels_age = {0: 'Adolescence', 1: 'Adult',2:'Child',3:'Senior Citizen'}\n",
    "        gender_model = tf.keras.models.load_model('./model/GenderDetection/Freeze/Freeze_BestModelGender.h5')\n",
    "        labels_gender = {0: 'Female', 1: 'Male'}\n",
    "        for i in range(len(st.session_state.heads)):\n",
    "            if (st.session_state.headsbool[i] == True):\n",
    "                col1, col2 = st.columns([1, 2])\n",
    "                img = st.session_state.heads[i]\n",
    "#                 img = img/255\n",
    "                resized = cv2.resize(img, (224,224))\n",
    "                reshaped = resized.reshape(1,224, 224,3)\n",
    "                reshaped=preprocess_input(reshaped)\n",
    "            \n",
    "                predictions = age_model(reshaped).numpy()\n",
    "                predicted_class = np.argmax(predictions,axis=1).item(0)\n",
    "                age_predicted_label = labels_age[predicted_class]\n",
    "                age_df = pd.DataFrame(predictions, columns=[labels_age[key] for key in labels_age])\n",
    "                \n",
    "                predictions = gender_model(reshaped).numpy()\n",
    "                predicted_class = np.argmax(predictions,axis=1).item(0)\n",
    "                gender_predicted_label = labels_gender[predicted_class]\n",
    "                gender_df = pd.DataFrame(predictions, columns=[labels_gender[key] for key in labels_gender])\n",
    "                with col1:\n",
    "                    st.image(st.session_state.heads[i])\n",
    "                with col2:\n",
    "#                     st.write(age_predicted_label)\n",
    "                    with st.expander(age_predicted_label):\n",
    "                        st.write(age_df)\n",
    "                    with st.expander(gender_predicted_label):\n",
    "                        st.write(gender_df)\n",
    "#                     st.write(gender_predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a430d6b",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangl\\anaconda3\\lib\\site-packages\\traitlets\\traitlets.py:2196: FutureWarning: Supporting extra quotes around Unicode is deprecated in traitlets 5.0. Use 'remove_cell' instead of \"'remove_cell'\" – or use CUnicode.\n",
      "  warn(\n",
      "[NbConvertApp] Converting notebook Main.ipynb to script\n",
      "[NbConvertApp] Writing 9376 bytes to Main.py\n"
     ]
    }
   ],
   "source": [
    "# before use streamlit:\n",
    "# conda install -c conda-forge nbconvert\n",
    "# run this cell (for convert ipynb to py)\n",
    "    # run this cmd in anaconda powershell prompt streamlit run .\\Main.py (remember change path to this file's location)\n",
    "!jupyter nbconvert Main.ipynb --to script --TagRemovePreprocessor.enabled=True --ClearMetadataPreprocessor.enabled=True -TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}'\n",
    "\n",
    "# !heroku create\n",
    "# git init\n",
    "# git add .\n",
    "# git commit -m \"initial commit\"\n",
    "# heroku create\n",
    "# git push heroku master"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
